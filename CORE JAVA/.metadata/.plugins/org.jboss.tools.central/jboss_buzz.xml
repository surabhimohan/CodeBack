<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Move your APIs into the serverless era with Camel K and Knative</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/4H4imgoWMFQ/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Knative" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><category term="service mesh" scheme="searchisko:content:tags" /><author><name>Abdellatif BOUCHAMA</name></author><id>searchisko:content:id:jbossorg_blog-move_your_apis_into_the_serverless_era_with_camel_k_and_knative</id><updated>2019-12-18T08:00:53Z</updated><published>2019-12-18T08:00:53Z</published><content type="html">&lt;p&gt;In the past few years, developers have addressed the challenge of evolving from monolith systems to &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices architecture&lt;/a&gt;. These days, we hear about the adoption of &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;&lt;em&gt;serverless&lt;/em&gt;&lt;/a&gt; systems.&lt;/p&gt; &lt;p&gt;Like many trends in software, there’s no one clear view of how to define serverless or how this approach offers added value for our software architecture. The perfect place to start with serverless systems and discover serverless capabilities is through a use case.&lt;span id="more-658557"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;In this video, we&amp;#8217;ll show how to move your APIs into the serverless era using the super duo of Camel K and &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;Knative&lt;/a&gt;:&lt;/p&gt; &lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/lOqubmVSGdw?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt; &lt;p&gt;By the end of this video, you will be able to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deploy an API with &lt;a href="https://camel.apache.org/camel-k/latest/index.html" target="_blank" rel="noopener noreferrer"&gt;Camel K&lt;/a&gt; as a &lt;a href="https://knative.dev" target="_blank" rel="noopener noreferrer"&gt;Knative&lt;/a&gt; service.&lt;/li&gt; &lt;li&gt;Deploy multiple revisions of a service.&lt;/li&gt; &lt;li&gt;Understand the scale-to-zero feature, and manage an auto-scaling strategy.&lt;/li&gt; &lt;li&gt;Set traffic distribution using different revisions of a service.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#38;linkname=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fmove-your-apis-into-the-serverless-era-with-camel-k-and-knative%2F&amp;#038;title=Move%20your%20APIs%20into%20the%20serverless%20era%20with%20Camel%20K%20and%20Knative" data-a2a-url="https://developers.redhat.com/blog/2019/12/18/move-your-apis-into-the-serverless-era-with-camel-k-and-knative/" data-a2a-title="Move your APIs into the serverless era with Camel K and Knative"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/18/move-your-apis-into-the-serverless-era-with-camel-k-and-knative/"&gt;Move your APIs into the serverless era with Camel K and Knative&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/4H4imgoWMFQ" height="1" width="1" alt=""/&gt;</content><summary>In the past few years, developers have addressed the challenge of evolving from monolith systems to microservices architecture. These days, we hear about the adoption of serverless systems. Like many trends in software, there’s no one clear view of how to define serverless or how this approach offers added value for our software architecture. The perfect place to start with serverless systems and ...</summary><dc:creator>Abdellatif BOUCHAMA</dc:creator><dc:date>2019-12-18T08:00:53Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/18/move-your-apis-into-the-serverless-era-with-camel-k-and-knative/</feedburner:origLink></entry><entry><title>Set up Red Hat AMQ Streams custom certificates on OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/sL6DyU57RiE/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><category term="TLS" scheme="searchisko:content:tags" /><author><name>Federico Valeri</name></author><id>searchisko:content:id:jbossorg_blog-set_up_red_hat_amq_streams_custom_certificates_on_openshift</id><updated>2019-12-18T08:00:10Z</updated><published>2019-12-18T08:00:10Z</published><content type="html">&lt;p&gt;Secure communication over a computer network is one of the most important requirements for a system, and yet it can be difficult to set up correctly. This example shows how to set up &lt;a href="https://developers.redhat.com/blog/category/stream-processing/" rel="noopener noreferrer"&gt;Red Hat AMQ Streams&lt;/a&gt;&amp;#8216; end-to-end TLS encryption using a custom X.509 CA certificate on the &lt;a href="http://developers.redhat.com/openshift/" target="_blank" rel="noopener noreferrer"&gt;Red Hat OpenShift&lt;/a&gt; platform.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You need to have the following in place before you can proceed with this example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An OpenShift cluster up and running with at least four CPUs and 5GB of memory.&lt;/li&gt; &lt;li&gt;A custom X.509 CA certificate in PEM format (along with its chain).&lt;/li&gt; &lt;li&gt;An active &lt;a href="https://access.redhat.com/" target="_blank" rel="noopener noreferrer"&gt;Red Hat Customer Portal&lt;/a&gt; account.&lt;/li&gt; &lt;li&gt;The &lt;a href="https://access.redhat.com/jbossnetwork/restricted/softwareDetail.html?softwareId=74481&amp;#38;product=jboss.amq.streams&amp;#38;version=1.3.0&amp;#38;downloadType=distributions" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams 1.3.0 Installation and Example package&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;An OpenShift user with the &lt;code&gt;cluster-admin&lt;/code&gt; role.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span id="more-662667"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;The procedure&lt;/h2&gt; &lt;p&gt;Before we start, let&amp;#8217;s define a few handy variables:&lt;/p&gt; &lt;pre&gt;USER="developer" PROJECT="streams" CA_USER="system:admin" RA_SECRET="reg-auth-secret" CLUSTER="my-cluster" &lt;/pre&gt; &lt;h3&gt;Set up a new project&lt;/h3&gt; &lt;p&gt;The first step after this is to log in as &lt;code&gt;cluster-admin&lt;/code&gt; and create a new project to host our clusters. We need this role because we have to install &lt;i&gt;custom resource definitions (CRDs)&lt;/i&gt; that are required by the &lt;i&gt;Cluster Operator (CO)&lt;/i&gt;. We then give full admin rights to the user to let them manage the project once ready:&lt;/p&gt; &lt;pre&gt;$ oc login -u $CA_USER $ oc new-project $PROJECT $ oc adm policy add-role-to-user admin $USER &lt;/pre&gt; &lt;p&gt;To be able to download images from the &lt;i&gt;Red Hat Container Registry&lt;/i&gt;, we also need to add an authentication Secret (use your credentials here):&lt;/p&gt; &lt;pre&gt;$ oc create secret docker-registry $RA_SECRET \ --docker-server=registry.redhat.io \ --docker-username=&amp;#60;portal-username&amp;#62; \ --docker-password=&amp;#60;portal-password&amp;#62; $ oc secrets link default $RA_SECRET --for=pull &lt;/pre&gt; &lt;p&gt;Then, unzip the &lt;i&gt;Installation and Examples&lt;/i&gt; distribution package and replace the default project&amp;#8217;s name with yours:&lt;/p&gt; &lt;pre&gt;TMP="/tmp/$PROJECT" &amp;#38;&amp;#38; rm -rf $TMP &amp;#38;&amp;#38; mkdir -p $TMP $ unzip -qq amq-streams-1.3.0-ocp-install-examples.zip -d $TMP $ sed -i -e "s/namespace: .*/namespace: $PROJECT/g" $TMP/install/cluster-operator/*RoleBinding*.yaml &lt;/pre&gt; &lt;p&gt;Now, we are ready to install all required CRDs and the Strimzi CO:&lt;/p&gt; &lt;pre&gt;$ oc apply -f $TMP/install/cluster-operator $ oc secrets link strimzi-cluster-operator $RA_SECRET --for=pull $ oc set env deploy/strimzi-cluster-operator STRIMZI_IMAGE_PULL_SECRETS=$RA_SECRET $ oc set env deploy/strimzi-cluster-operator STRIMZI_NAMESPACE=$PROJECT $ oc apply -f $TMP/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml $ oc apply -f $TMP/install/cluster-operator/032-RoleBinding-strimzi-cluster-operator-topic-operator-delegation.yaml $ oc apply -f $TMP/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml $ oc apply -f $TMP/install/strimzi-admin $ oc adm policy add-cluster-role-to-user strimzi-admin $USER &lt;/pre&gt; &lt;h3&gt;Configure the custom certificate&lt;/h3&gt; &lt;p&gt;After these commands finish, we can configure our custom X.509 CA certificate. I expect that you already have the following files:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;&lt;code&gt;rootca.pem&lt;/code&gt;&lt;/b&gt;: The root Certificate Authority (CA) in our domain (optional).&lt;/li&gt; &lt;li&gt;&lt;b&gt;&lt;code&gt;intermca.pem&lt;/code&gt;&lt;/b&gt;: An intermediate CA used to sign the certificate in a specific context (optional).&lt;/li&gt; &lt;li&gt;&lt;b&gt;&lt;code&gt;myca.pem&lt;/code&gt;&lt;/b&gt;: Our custom CA certificate to use with Apache Kafka.&lt;/li&gt; &lt;li&gt;&lt;b&gt;&lt;code&gt;myca-prk.pem&lt;/code&gt;&lt;/b&gt;: The private key for our custom CA certificate.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All CAs in the chain should be configured as a CA in the X509v3 Basic Constraints. This means that &lt;em&gt;you cannot use a classic non-CA certificate to replace the self-generated certificate&lt;/em&gt; (see also additional notes at the end). The reason for this is that it is used to sign certificates for inter-broker communication.&lt;/p&gt; &lt;p&gt;After printing out your custom certificate you should be able to see this property:&lt;/p&gt; &lt;pre&gt;$ openssl x509 -inform pem -in myca.pem -noout -text ... X509v3 Basic Constraints: CA:TRUE &lt;/pre&gt; &lt;p&gt;When you have a valid CA certificate, create a bundle file like this:&lt;/p&gt; &lt;pre&gt;$ cat myca.pem intermca.pem rootca.pem &amp;#62; bundle.pem &lt;/pre&gt; &lt;p&gt;Then, create all required Secrets and labels containing our custom CA. This must be done before creating our custom cluster (next step):&lt;/p&gt; &lt;pre&gt;$ oc create secret generic $CLUSTER-cluster-ca-cert --from-file=ca.crt=bundle.pem $ oc label secret $CLUSTER-cluster-ca-cert strimzi.io/kind=Kafka strimzi.io/cluster=$CLUSTER $ oc create secret generic $CLUSTER-cluster-ca --from-file=ca.key=myca-prk.pem $ oc label secret $CLUSTER-cluster-ca strimzi.io/kind=Kafka strimzi.io/cluster=$CLUSTER $ oc create secret generic $CLUSTER-clients-ca-cert --from-file=ca.crt=bundle.pem $ oc label secret $CLUSTER-clients-ca-cert strimzi.io/kind=Kafka strimzi.io/cluster=$CLUSTER $ oc create secret generic $CLUSTER-clients-ca --from-file=ca.key=myca-prk.pem $ oc label secret $CLUSTER-clients-ca strimzi.io/kind=Kafka strimzi.io/cluster=$CLUSTER &lt;/pre&gt; &lt;p&gt;Finally, we can deploy our cluster definition. Note how we set &lt;code&gt;generateCertificateAuthority&lt;/code&gt; to instruct the CO not to generate the self-signed CA that otherwise would overwrite our previous configuration.&lt;/p&gt; &lt;h3&gt;Example: Ephemeral cluster creation (not for production)&lt;/h3&gt; &lt;p&gt;Here we create a small ephemeral cluster just for the sake of this example. &lt;em&gt;Do not use the exact same setup for production&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt;$ oc create -f - &amp;#60;&amp;#60;EOF apiVersion: kafka.strimzi.io/v1alpha1 kind: Kafka metadata: name: $CLUSTER spec: kafka: version: "2.3.0" replicas: 1 config: num.partitions: 1 default.replication.factor: 1 log.message.format.version: "2.3" clusterCa: generateCertificateAuthority: false clientsCa: generateCertificateAuthority: false listeners: plain: {} tls: {} external: type: route readinessProbe: initialDelaySeconds: 30 timeoutSeconds: 10 livenessProbe: initialDelaySeconds: 30 timeoutSeconds: 10 template: pod: terminationGracePeriodSeconds: 120 storage: type: ephemeral resources: requests: cpu: "1000m" memory: "2Gi" limits: cpu: "1000m" memory: "2Gi" tlsSidecar: resources: limits: cpu: "100m" memory: "128Mi" requests: cpu: "100m" memory: "128Mi" zookeeper: replicas: 1 readinessProbe: initialDelaySeconds: 15 timeoutSeconds: 5 livenessProbe: initialDelaySeconds: 15 timeoutSeconds: 5 storage: type: ephemeral resources: requests: cpu: "500m" memory: "1Gi" limits: cpu: "500m" memory: "1Gi" tlsSidecar: resources: limits: cpu: "100m" memory: "128Mi" requests: cpu: "100m" memory: "128Mi" entityOperator: topicOperator: resources: limits: cpu: "250m" memory: "256Mi" requests: cpu: "250m" memory: "256Mi" userOperator: resources: limits: cpu: "250m" memory: "256Mi" requests: cpu: "250m" memory: "256Mi" tlsSidecar: resources: limits: cpu: "100m" memory: "128Mi" requests: cpu: "100m" memory: "128Mi" EOF &lt;/pre&gt; &lt;p&gt;Once the cluster is up and running, you might want to check that the custom CA is correctly loaded:&lt;/p&gt; &lt;pre&gt;$ oc get pods $ oc logs strimzi-cluster-operator-&amp;#60;uuid&amp;#62; $ oc logs $CLUSTER-kafka-0 -c kafka &lt;/pre&gt; &lt;h3&gt;Set up the Java client&lt;/h3&gt; &lt;p&gt;Create and use a truststore in Java KeyStore (JKS) format for one-way TLS authentication:&lt;/p&gt; &lt;pre&gt;$ oc extract secret/$CLUSTER-cluster-ca-cert --keys=ca.crt --to=- &amp;#62; ca.pem keytool -import -noprompt -alias root -file ca.pem -keystore truststore.jks -storepass secret &lt;/pre&gt; &lt;p&gt;If you want to access Kafka from outside OpenShift, then you also need to use this bootstrap URL:&lt;/p&gt; &lt;pre&gt;$ echo $(oc get routes $CLUSTER-kafka-bootstrap -o=jsonpath='{.status.ingress[0].host}{"\n"}'):443 &lt;/pre&gt; &lt;h2&gt;Additional notes&lt;/h2&gt; &lt;p&gt;We already know that most security teams won&amp;#8217;t easily release CA certificates. We are working on an &lt;a href="https://issues.jboss.org/browse/ENTMQST-1371" target="_blank" rel="noopener noreferrer"&gt;enhancement&lt;/a&gt; to provide the option to use a non-CA certificate for Kafka listeners, leaving the internal self-generated CA to secure the inter-broker communication.&lt;/p&gt; &lt;p&gt;Beware that when using a custom CA as explained in this post, you are responsible for the certificate renewals. This process is fully automated when using self-generated certificates. In any case, after the renewal, you will have to recreate the client&amp;#8217;s truststore as described before.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#38;linkname=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F18%2Fset-up-red-hat-amq-streams-custom-certificates-on-openshift%2F&amp;#038;title=Set%20up%20Red%20Hat%20AMQ%20Streams%20custom%20certificates%20on%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2019/12/18/set-up-red-hat-amq-streams-custom-certificates-on-openshift/" data-a2a-title="Set up Red Hat AMQ Streams custom certificates on OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/18/set-up-red-hat-amq-streams-custom-certificates-on-openshift/"&gt;Set up Red Hat AMQ Streams custom certificates on OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/sL6DyU57RiE" height="1" width="1" alt=""/&gt;</content><summary>Secure communication over a computer network is one of the most important requirements for a system, and yet it can be difficult to set up correctly. This example shows how to set up Red Hat AMQ Streams‘ end-to-end TLS encryption using a custom X.509 CA certificate on the Red Hat OpenShift platform. Prerequisites You need to have the following in place before you can proceed with this example: An ...</summary><dc:creator>Federico Valeri</dc:creator><dc:date>2019-12-18T08:00:10Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/18/set-up-red-hat-amq-streams-custom-certificates-on-openshift/</feedburner:origLink></entry><entry><title>The blog is moving!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/k4b9rwQBHqY/the-blog-is-moving" /><category term="feed_group_name_resteasy" scheme="searchisko:content:tags" /><category term="feed_name_resteasy" scheme="searchisko:content:tags" /><author><name>Alessio Soldano</name></author><id>searchisko:content:id:jbossorg_blog-the_blog_is_moving</id><updated>2019-12-17T11:02:04Z</updated><published>2019-12-17T11:01:00Z</published><content type="html">&lt;!-- [DocumentBodyStart:3f114bab-c7d1-4e04-87cd-3fe2a61ae71e] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;The RESTEasy project blog is moving to a &lt;a class="jive-link-external-small" href="https://resteasy.github.io/blogs/" rel="nofollow"&gt;new location&lt;/a&gt; and this is going to be the last post here.&lt;/p&gt;&lt;p&gt;As you might have noticed, the website (currently hosted on &lt;a class="jive-link-external-small" href="https://github.com/resteasy/resteasy.github.io" rel="nofollow"&gt;GitHub&lt;/a&gt;) has just been updated to include the blog too.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Please keep on following us at &lt;a class="jive-link-external-small" href="https://resteasy.github.io/blogs/" rel="nofollow"&gt;https://resteasy.github.io/blogs/&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:3f114bab-c7d1-4e04-87cd-3fe2a61ae71e] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/k4b9rwQBHqY" height="1" width="1" alt=""/&gt;</content><summary>The RESTEasy project blog is moving to a new location and this is going to be the last post here. As you might have noticed, the website (currently hosted on GitHub) has just been updated to include the blog too.   Please keep on following us at https://resteasy.github.io/blogs/</summary><dc:creator>Alessio Soldano</dc:creator><dc:date>2019-12-17T11:01:00Z</dc:date><feedburner:origLink>https://developer.jboss.org/community/resteasy/blog/2019/12/17/the-blog-is-moving</feedburner:origLink></entry><entry><title>Deploying debuginfod servers for your developers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Q0eZjt6pN44/" /><category term="C" scheme="searchisko:content:tags" /><category term="C++" scheme="searchisko:content:tags" /><category term="debugging" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="observability" scheme="searchisko:content:tags" /><category term="performance" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Frank Eigler</name></author><id>searchisko:content:id:jbossorg_blog-deploying_debuginfod_servers_for_your_developers</id><updated>2019-12-17T08:00:52Z</updated><published>2019-12-17T08:00:52Z</published><content type="html">&lt;p&gt;In an &lt;a href="https://developers.redhat.com/blog/2019/10/14/introducing-debuginfod-the-elfutils-debuginfo-server/"&gt;earlier article&lt;/a&gt;, Aaron Merey introduced the new elfutils &lt;code&gt;debuginfo-server&lt;/code&gt; daemon. With this software now integrated and released into elfutils 0.178 and coming to distros near you, it&amp;#8217;s time to consider why and how to set up such a service for yourself and your team.&lt;/p&gt; &lt;p&gt;Recall that &lt;code&gt;debuginfod&lt;/code&gt; exists to distribute ELF or DWARF debugging information, plus associated source code, for a collection of binaries. If you need to run a debugger like &lt;code&gt;gdb&lt;/code&gt;, a trace or probe tool like &lt;code&gt;perf&lt;/code&gt; or &lt;code&gt;systemtap&lt;/code&gt;, binary analysis tools like &lt;code&gt;binutils&lt;/code&gt; or &lt;code&gt;pahole&lt;/code&gt;, or binary rewriting libraries like &lt;code&gt;dyninst&lt;/code&gt;, you will eventually need &lt;code&gt;debuginfo&lt;/code&gt; that matches your binaries. The &lt;code&gt;debuginfod&lt;/code&gt; client support in these tools enables a fast, transparent way of fetching this data on the fly, without ever having to stop, change to root, run all of the right &lt;code&gt;yum debuginfo-install&lt;/code&gt; commands, and try again. Debuginfo lets you debug anywhere, anytime.&lt;/p&gt; &lt;p&gt;We hope this opening addresses the &amp;#8220;why.&amp;#8221; Now, onto the &amp;#8220;how.&amp;#8221;&lt;span id="more-658327"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Basic server operation&lt;/h2&gt; &lt;p&gt;For clients to be able to download content, you need one or more &lt;code&gt;debuginfod&lt;/code&gt; servers, each with access to all of the potentially needed &lt;code&gt;debuginfo&lt;/code&gt;. Ideally, you should run &lt;code&gt;debuginfod&lt;/code&gt; servers as close as possible to the machines holding a copy of those build artifacts.&lt;/p&gt; &lt;p&gt;If you build your own software, then its build and source trees are in one location. To run a copy of &lt;code&gt;debuginfod&lt;/code&gt; on your build machines:&lt;/p&gt; &lt;pre&gt;$ debuginfod -F /path/to/build/tree1 /path/to/build/tree2 &lt;/pre&gt; &lt;p&gt;Then, &lt;code&gt;debuginfod&lt;/code&gt; will periodically rescan all of these trees and make available all of the executables and debugging data there, plus the source files referenced from there. If you rebuild your code, the index will catch up soon (see the &lt;code&gt;-t&lt;/code&gt; parameter).&lt;/p&gt; &lt;p&gt;If you build your own software all the way into RPMs, then run a copy of &lt;code&gt;debuginfod&lt;/code&gt; with the parent directories containing the RPM files:&lt;/p&gt; &lt;pre&gt;$ debuginfod -R /path/to/rpm/tree1 /path/to/rpm/tree2 &lt;/pre&gt; &lt;p&gt;Then, &lt;code&gt;debuginfod&lt;/code&gt; will periodically rescan all of these trees and make available all of the executables and the debugging files inside the RPMs. This tool matches &lt;code&gt;-debuginfo&lt;/code&gt; and &lt;code&gt;-debugsource&lt;/code&gt; files automatically.&lt;/p&gt; &lt;p&gt;Naturally, you can do both with one &lt;code&gt;debuginfod&lt;/code&gt; process: Just add those arguments together.&lt;/p&gt; &lt;p&gt;If you need to debug software that&amp;#8217;s a part of your Linux distribution, you have a bit of a quandary. Until distributions set up public &lt;code&gt;debuginfod&lt;/code&gt; servers, we have to fend for ourselves. Luckily, doing this is not too difficult. After all, you just need a machine where the distro&amp;#8217;s relevant packages have been installed—or even just downloaded:&lt;/p&gt; &lt;pre&gt;$ mkdir distro-rpms ; cd distro-rpms $ debuginfod -R . &lt;/pre&gt; &lt;p&gt;and repeat as needed:&lt;/p&gt; &lt;pre&gt;$ yumdownloader PACKAGE-N-V-R $ yumdownloader --debuginfo PACKAGE-N-V-R &lt;/pre&gt; &lt;p&gt;with all of the wildcards and retention that your disk will permit.&lt;/p&gt; &lt;p&gt;If you are running a &lt;a href="https://www.redhat.com/en/technologies/management/satellite" rel="noopener noreferrer"&gt;Red Hat Satellite&lt;/a&gt; server in-house, or an informally managed mirror of distro packages, you can run &lt;code&gt;debuginfod&lt;/code&gt; against those systems&amp;#8217; package archives in situ. There&amp;#8217;s no need to install (&lt;code&gt;rpm -i&lt;/code&gt;), filter, or reorganize them in any artificial way. Just let a copy of &lt;code&gt;debuginfod&lt;/code&gt; scan the directories.&lt;/p&gt; &lt;h2&gt;Client configuration&lt;/h2&gt; &lt;p&gt;OK, you now have one or more servers running, and they can be scanning the same or different trees of &lt;code&gt;debuginfo&lt;/code&gt; material. How do we get the clients to talk to them? The simple and obvious solution is to enumerate all of the servers you know of:&lt;/p&gt; &lt;pre&gt;$ export DEBUGINFOD_URLS="http://host1:8002/ http://host2:8002/ ...." $ gdb ... etc. &lt;/pre&gt; &lt;p&gt;For every lookup, the client will send a query to all of the servers at once, and the first one that reports back with the requested information will &amp;#8220;win.&amp;#8221;&lt;/p&gt; &lt;p&gt;While this tactic works, there are a couple of downsides. First, one has to propagate this list of URLs to every client. Second, there is no opportunity to centrally cache content, so each client has to download content separately from the origin server (in HTTP terminology). There is a simple fix: federation.&lt;/p&gt; &lt;p&gt;Each &lt;code&gt;debuginfod&lt;/code&gt; server can also act as a client. If the server can&amp;#8217;t answer a query from its local index and has been configured with a list of upstream &lt;code&gt;$DEBUGINFO_URLS&lt;/code&gt;, then it will forward the request to the upstream servers. It will then cache the result of a positive response and then relay it back. The next request to the same object will be served from the cache (subject to cache retention constraints), instead.&lt;/p&gt; &lt;p&gt;This behavior lets you configure a federated hierarchy of &lt;code&gt;debuginfod&lt;/code&gt; servers. Doing so allows the concentration of configuration files and localizes caching. Each of your per-build-system &lt;code&gt;debuginfods&lt;/code&gt; can then be configured with a list of its higher-level peers. You can even have &lt;code&gt;debuginfod&lt;/code&gt; servers that don&amp;#8217;t scan any local directories at all, but function purely as upstream relays. Make sure the federation is a tree or directed-acylic-graph. Cycles would be bad.&lt;/p&gt; &lt;h2&gt;Server management&lt;/h2&gt; &lt;p&gt;You now have one or more servers running, and clients depending on them. What about keeping them running well? There are a couple of practical issues to worry about.&lt;/p&gt; &lt;p&gt;One is resource usage during and after indexing. Initial &lt;code&gt;debuginfod&lt;/code&gt; indexing is intense on the CPU and storage. It must momentarily stream-decompress RPMs, and parse every ELF or DWARF file. The index database is a tightly formatted SQLite file, but it can grow to around 1% of the size of normal compressed RPMs. If this aspect is not a problem, then don&amp;#8217;t worry about this next paragraph.&lt;/p&gt; &lt;p&gt;If indexing time and space for a very large set of archives is excessive, it can be helpful to run &lt;code&gt;debuginfod&lt;/code&gt; with file filters. Its &lt;code&gt;-I&lt;/code&gt; and &lt;code&gt;-X&lt;/code&gt; options let you specify regular expressions for file names that it should include or exclude. Say that, for example, your archive has multiple and different intermingled architectures or different major distro versions of files, and you only want to track a subset. You can use these options to force &lt;code&gt;debuginfod&lt;/code&gt; to skip files whose names don&amp;#8217;t match the patterns:&lt;/p&gt; &lt;pre&gt;$ debuginfod -I '\.el[78]\.x86_64' -X 'python' -R /path &lt;/pre&gt; &lt;p&gt;If your server has a lot of cores, consider splitting up the scan path into numerous subpaths, because &lt;code&gt;debuginfod&lt;/code&gt; starts one or two threads per path given on the command line. Actual concurrency is carefully managed, so you can be carefree when giving large path lists. So, use:&lt;/p&gt; &lt;pre&gt;$ debuginfod -R /path/*&lt;/pre&gt; &lt;p&gt;instead of&lt;/p&gt; &lt;pre&gt;$ debuginfod -R /path&lt;/pre&gt; &lt;p&gt;If your ELF, DWARF, or RPM archive is very large, you might consider sharding the scanning task between multiple copies of &lt;code&gt;debuginfod,&lt;/code&gt;each running near the storage server. You can use wildcards plus include and exclude paths to give each &lt;code&gt;debuginfod&lt;/code&gt; process only a subset of the data. We discussed above how &lt;code&gt;debuginfod&lt;/code&gt; servers can federate. Use that facility to create a single front-end &lt;code&gt;debuginfod&lt;/code&gt; that scans nothing, but delegates queries to the entire stableful of shards.&lt;/p&gt; &lt;p&gt;Running network servers in a shell by hand is a fine old-school method for playing around. For serious deployments, though, you will want your &lt;code&gt;debuginfod&lt;/code&gt; server to be managed by a supervisory system. Because &lt;code&gt;debuginfod&lt;/code&gt; runs so nicely in a plain shell, it runs just as nicely as a &lt;code&gt;systemd&lt;/code&gt; service or inside a container. A sample &lt;code&gt;systemd&lt;/code&gt; configuration comes with elfutils, and we plan to publish dockerfiles or container images with which you can run debuginfod inside &lt;a href="http://developers.redhat.com/openshift/" rel="noopener"&gt;Red Hat OpenShift&lt;/a&gt;, or another orchestration service.&lt;/p&gt; &lt;p&gt;Once the server is running, it&amp;#8217;s good to monitor it to keep it running. Textual logs go to standard output and error streams, where tools like &lt;code&gt;systemd&lt;/code&gt; journal or OpenShift can collect the text. Add more &lt;code&gt;-v&lt;/code&gt; verbosity options to generate more detailed traces. In addition to this textual data, &lt;code&gt;debuginfod&lt;/code&gt; serves a &lt;code&gt;/metrics&lt;/code&gt; web API URL, which is a Prometheus export-formatted quantitative data source. This URL provides internal statistics about what the server&amp;#8217;s threads are up to. It would not be hard to wire up alerting systems or other programs to detect various types of anomalies.&lt;/p&gt; &lt;p&gt;Security becomes a concern as soon as a &lt;code&gt;debuginfod&lt;/code&gt; service is provided across trust boundaries, such as on the internet and to the public. The man page offers a plethora of caution about the measures required for such a service to be safe for the users as well as the service operator. It&amp;#8217;s not rocket science, but ordinary HTTP frontend protections such as TLS encryption and load control are a must, such as using an HAProxy installation. It is also important to limit &lt;code&gt;debuginfod&lt;/code&gt; indexing to trustworthy (non-hostile) binaries.&lt;/p&gt; &lt;h3&gt;Looking ahead&lt;/h3&gt; &lt;p&gt;What does the future hold? We&amp;#8217;d like to support Debian format packages soon, so our friends in that ecosystem can also take full advantage. We would be delighted to assist Linux distributions in operating public &lt;code&gt;debuginfod&lt;/code&gt; services for their users and are already prototyping this service in &lt;a href="https://koji.fedoraproject.org/koji/" target="_blank" rel="noopener noreferrer"&gt;Fedora koji&lt;/a&gt;. We also envision more manageability features, and perhaps integration with source version control systems. We also welcome suggestions from our early adopters—you!&lt;/p&gt; &lt;p&gt;We hope this article was helpful in motivating you and helping you set up your own &lt;code&gt;debuginfod&lt;/code&gt; services. Please contact us on the &lt;a href="mailto:elfutils-devel@sourceware.org" target="_blank" rel="noopener noreferrer"&gt;elfutils-devel@sourceware.org&lt;/a&gt; mailing list.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#38;linkname=Deploying%20debuginfod%20servers%20for%20your%20developers" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Fdeploying-debuginfod-servers-for-your-developers%2F&amp;#038;title=Deploying%20debuginfod%20servers%20for%20your%20developers" data-a2a-url="https://developers.redhat.com/blog/2019/12/17/deploying-debuginfod-servers-for-your-developers/" data-a2a-title="Deploying debuginfod servers for your developers"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/17/deploying-debuginfod-servers-for-your-developers/"&gt;Deploying debuginfod servers for your developers&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Q0eZjt6pN44" height="1" width="1" alt=""/&gt;</content><summary>In an earlier article, Aaron Merey introduced the new elfutils debuginfo-server daemon. With this software now integrated and released into elfutils 0.178 and coming to distros near you, it’s time to consider why and how to set up such a service for yourself and your team. Recall that debuginfod exists to distribute ELF or DWARF debugging information, plus associated source code, for a collection ...</summary><dc:creator>Frank Eigler</dc:creator><dc:date>2019-12-17T08:00:52Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/17/deploying-debuginfod-servers-for-your-developers/</feedburner:origLink></entry><entry><title>Replacing Confluent Schema Registry with Red Hat integration service registry</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5a-FOUCGh6w/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><category term="Red Hat Integration" scheme="searchisko:content:tags" /><author><name>Hugo Guerrero</name></author><id>searchisko:content:id:jbossorg_blog-replacing_confluent_schema_registry_with_red_hat_integration_service_registry</id><updated>2019-12-17T08:00:12Z</updated><published>2019-12-17T08:00:12Z</published><content type="html">&lt;p&gt;With the latest release of &lt;a href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; now available, we’ve introduced some exciting new capabilities. Along the enhancements for Apache Kafka-based environments, Red Hat &lt;a href="https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/"&gt;announced&lt;/a&gt; the &lt;a href="https://access.redhat.com/support/offerings/techpreview"&gt;Technical Preview&lt;/a&gt; of the Red Hat Integration service registry to help teams to govern their services schemas. Developers can now use the registry to query for the schemas and artifacts required by each service endpoint or register and store new structures for future use.&lt;br /&gt; &lt;span id="more-662017"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Registry for event-driven architecture&lt;/h2&gt; &lt;p&gt;Red Hat Integration’s service registry, based on the &lt;a href="https://github.com/hguerrero/amq-examples/tree/master/registry-example-avro#apicurio-service-registry-example---avro"&gt;Apicurio project registry&lt;/a&gt;, provides a way to decouple the schema used to serialize and deserialize Kafka messages with the applications that are sending/receiving them. The service registry is a store for schema (and API design) artifacts providing a REST API and a set of optional rules for enforcing content validity and evolution. The registry handles data formats like Apache Avro, JSON Schema, Google Protocol Buffers (protobuf), as well as OpenAPI and AsyncAPI definitions.&lt;/p&gt; &lt;p&gt;To make it easy to transition from Confluent, the service registry added compatibility with the Confluent Schema Registry REST API. This means that applications using Confluent client libraries can replace Schema Registry and use Red Hat Integration service registry instead.&lt;/p&gt; &lt;h2&gt;Replacing Confluent Schema Registry&lt;/h2&gt; &lt;p&gt;&lt;img class="aligncenter size-large wp-image-661417" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/registry-client-1024x489.png" alt="" width="640" height="306" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/registry-client-1024x489.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/registry-client-300x143.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/registry-client-768x367.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/registry-client.png 1156w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;br /&gt; For the sake of simplicity in this article, I will use an existing &lt;a href="https://github.com/confluentinc/examples/tree/5.3.1-post/clients/avro"&gt;Avro client example&lt;/a&gt; already available to show you how to switch from Confluent Schema Registry to the Red Hat Integration service registry.&lt;/p&gt; &lt;p&gt;You will need to use &lt;a href="https://docs.docker.com/compose/"&gt;docker-compose&lt;/a&gt; for starting a local environment and Git for cloning the repository code.&lt;/p&gt; &lt;p&gt;1. Clone the example GitHub repository:&lt;/p&gt; &lt;pre&gt;$ git clone &lt;a href="https://github.com/confluentinc/examples.git"&gt;https://github.com/confluentinc/examples.git&lt;/a&gt; $ git checkout 5.3.1-post&lt;/pre&gt; &lt;p&gt;2. Change to the avro example folder:&lt;/p&gt; &lt;pre&gt;$ cd examples/clients/avro/&lt;/pre&gt; &lt;p&gt;3. Open the &lt;code&gt;ConsumerExample.java&lt;/code&gt; file under &lt;code&gt;src/main/java/io/confluent/examples/clients/basicavro&lt;/code&gt;.&lt;br /&gt; 4. Replace the SCHEMA_REGISTRY_URL_CONFIG property with the following:&lt;/p&gt; &lt;pre&gt;... props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "&lt;strong&gt;http://localhost:8081/confluent&lt;/strong&gt;"); ...&lt;/pre&gt; &lt;p&gt;5. Repeat the last step with the &lt;code&gt;ProducerExample.java&lt;/code&gt; file.&lt;br /&gt; 6. Download this &lt;a href="https://github.com/hguerrero/amq-examples/tree/master/registry-example-avro#apicurio-service-registry-example---avro" target="_blank" rel="noopener noreferrer"&gt;docker-compose.yaml&lt;/a&gt; file example to deploy a simple Kafka cluster with the Apicurio registry.&lt;br /&gt; 7. Start the Kafka cluster and registry.&lt;/p&gt; &lt;pre&gt;$ docker-compose -f docker-compose.yaml up&lt;/pre&gt; &lt;p&gt;8. To run the producer, compile the project:&lt;/p&gt; &lt;pre&gt;$ mvn clean compile package&lt;/pre&gt; &lt;p&gt;9. Run &lt;code&gt;ProducerExample.java&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ mvn exec:java -Dexec.mainClass=io.confluent.examples.clients.basicavro.ProducerExample&lt;/pre&gt; &lt;p&gt;10. After a few moments you should see the following output:&lt;/p&gt; &lt;pre&gt;... Successfully produced 10 messages to a topic called transactions [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ ...&lt;/pre&gt; &lt;p&gt;11. Now run the consumer:&lt;/p&gt; &lt;pre&gt;$ mvn exec:java -Dexec.mainClass=io.confluent.examples.clients.basicavro.ConsumerExample&lt;/pre&gt; &lt;p&gt;The messages should be displayed on your screen:&lt;/p&gt; &lt;pre&gt;... offset = 0, key = id0, value = {"id": "id0", "amount": 1000.0} offset = 1, key = id1, value = {"id": "id1", "amount": 1000.0} offset = 2, key = id2, value = {"id": "id2", "amount": 1000.0} offset = 3, key = id3, value = {"id": "id3", "amount": 1000.0} offset = 4, key = id4, value = {"id": "id4", "amount": 1000.0} offset = 5, key = id5, value = {"id": "id5", "amount": 1000.0} offset = 6, key = id6, value = {"id": "id6", "amount": 1000.0} offset = 7, key = id7, value = {"id": "id7", "amount": 1000.0} offset = 8, key = id8, value = {"id": "id8", "amount": 1000.0} offset = 9, key = id9, value = {"id": "id9", "amount": 1000.0} ...&lt;/pre&gt; &lt;p&gt;12. To check the schema that the producer added to the registry you can issue the following curl command:&lt;/p&gt; &lt;pre&gt;$ curl --silent -X GET http://localhost:8081/confluent/schemas/ids/1 | jq .&lt;/pre&gt; &lt;p&gt;13. The result should show you the Avro schema:&lt;/p&gt; &lt;pre&gt;{  "schema": "{\"type\":\"record\",\"name\":\"Payment\",\"namespace\":\"io.confluent.examples.clients.basicavro\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"amount\",\"type\":\"double\"}]}" }&lt;/pre&gt; &lt;p&gt;Done!&lt;/p&gt; &lt;p&gt;As you can see, you can just change the URL for the registry to use Red Hat service registry instead without the need to change any code in your applications.&lt;/p&gt; &lt;p&gt;If you are interested in other features of the Red Hat Integration service registry, you can see a full-fledged example using Quarkus Kafka extension in my &lt;a href="https://github.com/hguerrero/amq-examples"&gt;amq-examples GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The Red Hat Integration service registry is a central data store for schemas and API artifacts. Developers can query, create, read, update, and delete service artifacts, versions, and rules to govern the structure of their services. The service registry could be also used as a drop-in replacement for Confluent Schema Registry with Apache Kafka clients. With just a change to the URL for the registry, you can use Red Hat service registry without needing to change code in your applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;See also:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/"&gt;Red Hat advances Debezium CDC connectors for Apache Kafka support to Technical Preview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/"&gt;Red Hat simplifies transition to open source Kafka with new service registry and HTTP bridge&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F17%2Freplacing-confluent-schema-registry-with-red-hat-integration-service-registry%2F&amp;#038;title=Replacing%20Confluent%20Schema%20Registry%20with%20Red%20Hat%20integration%20service%20registry" data-a2a-url="https://developers.redhat.com/blog/2019/12/17/replacing-confluent-schema-registry-with-red-hat-integration-service-registry/" data-a2a-title="Replacing Confluent Schema Registry with Red Hat integration service registry"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/17/replacing-confluent-schema-registry-with-red-hat-integration-service-registry/"&gt;Replacing Confluent Schema Registry with Red Hat integration service registry&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5a-FOUCGh6w" height="1" width="1" alt=""/&gt;</content><summary>With the latest release of Red Hat Integration now available, we’ve introduced some exciting new capabilities. Along the enhancements for Apache Kafka-based environments, Red Hat announced the Technical Preview of the Red Hat Integration service registry to help teams to govern their services schemas. Developers can now use the registry to query for the schemas and artifacts required by each servi...</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2019-12-17T08:00:12Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/17/replacing-confluent-schema-registry-with-red-hat-integration-service-registry/</feedburner:origLink></entry><entry><title>VS Code Language support for Apache Camel 0.0.20 release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/h5rpEEmvtRo/" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat Fuse" scheme="searchisko:content:tags" /><category term="Red Hat Integration" scheme="searchisko:content:tags" /><category term="VS Code" scheme="searchisko:content:tags" /><category term="VS Code Extensions" scheme="searchisko:content:tags" /><author><name>Aurélien Pupier</name></author><id>searchisko:content:id:jbossorg_blog-vs_code_language_support_for_apache_camel_0_0_20_release</id><updated>2019-12-16T14:59:48Z</updated><published>2019-12-16T14:59:48Z</published><content type="html">&lt;p&gt;During the past months, several noticeable new features have been added to improve the developer experience of application based on &lt;a href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt;. These updates are available in the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-apache-camel"&gt;0.0.20 release of Visual Studio (VS) Code extension&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Before going into the list of updates in detail, I want to note that I mentioned in the title the VS Code Extension release because VS Code extension is covering the broader set of new features. Don&amp;#8217;t worry if you are using another IDE, though, most features are also available in all other IDEs that support the Camel Language Server (Eclipse Desktop, Eclipse Che, and more).&lt;/p&gt; &lt;p&gt;&lt;span id="more-666057"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Camel 3 inside&lt;/h2&gt; &lt;p&gt;Camel 3 was &lt;a href="https://camel.apache.org/blog/release-3-0-0.html"&gt;released&lt;/a&gt; a few weeks ago, and the Camel Language Server is already relying on it internally. What does that mean for the end user? It means that the default catalog is now using Camel 3.&lt;/p&gt; &lt;p&gt;If you are still based on Camel 2.x, no problem; check out the following awesome feature.&lt;/p&gt; &lt;h2&gt;Camel catalog version options&lt;/h2&gt; &lt;p&gt;A parameter is now available to choose the Camel catalog version that you want. This can be specified in &lt;em&gt;File -&amp;#62; Preferences -&amp;#62; Settings -&amp;#62; Apache Camel Tooling -&amp;#62; Camel catalog version.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-666097 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/CamelCatalogVersionPreference.gif" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/CamelCatalogVersionPreference.gif" alt="Camel Catalog Version Preference in VS Code" width="1023" height="595" /&gt;&lt;/p&gt; &lt;h2&gt;Diagnostics with quick fixes and more precise range&lt;/h2&gt; &lt;p&gt;Diagnostics are really useful to pinpoint issues in our code. In previous versions, the diagnostic range error was provided on the full Camel URI. Now, for invalid component parameter keys and invalid component parameter enum values, the range is more precise and is pointing to the exact incriminated key or value.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-666147 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screenshot-from-2019-12-12-16-39-50-1024x394.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screenshot-from-2019-12-12-16-39-50.png" alt="" width="1216" height="468" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screenshot-from-2019-12-12-16-39-50.png 1216w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screenshot-from-2019-12-12-16-39-50-300x115.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screenshot-from-2019-12-12-16-39-50-768x296.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screenshot-from-2019-12-12-16-39-50-1024x394.png 1024w" sizes="(max-width: 1216px) 100vw, 1216px" /&gt;&lt;/p&gt; &lt;p&gt;For unknown component parameter keys, there is also a quick fix provided if a component parameter key relatively similar exists. It is very useful in case of small typos.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-666157 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/filteredListOfQuickfix.gif" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/filteredListOfQuickfix.gif" alt="" width="989" height="463" /&gt;&lt;/p&gt; &lt;h2&gt;Additional Camel components&lt;/h2&gt; &lt;p&gt;If you are using Camel components that are not part of Camel core catalog, it is now possible to provide the Camel component definition to have it fully supported by the tooling. The Camel component definition is defined as JSON. The JSON file can be found in the jar of the Camel component. The preference can be specified through &lt;em&gt;File -&amp;#62; Preferences -&amp;#62; Settings -&amp;#62; Apache Camel Tooling -&amp;#62; Extra-components -&amp;#62; Edit in settings.json.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/U015RzlgFNM?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Properties file support&lt;/h2&gt; &lt;p&gt;Camel allows you to configure general component properties using a Properties file (see &lt;a href="https://github.com/apache/camel/blob/master/examples/camel-example-main/src/main/resources/application.properties#L42"&gt;here&lt;/a&gt; for an example). Completion is available for component ids and component property keys.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-666167 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/completionProperties.gif" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/completionProperties.gif" alt="" width="877" height="402" /&gt;&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s next?&lt;/h2&gt; &lt;p&gt;This is very opened for the future. There will surely be improvements directed to Apache Camel K support. The Fuse Tooling team is awaiting your feedback on &lt;a href="https://issues.redhat.com/browse/FUSETOOLS2"&gt;JIRA&lt;/a&gt;, on one of the &lt;a href="https://github.com/camel-tooling"&gt;Camel Tooling GitHub repository&lt;/a&gt;. or whatever channel you prefer. it will help drive the future roadmap.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#38;linkname=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fvs-code-language-support-for-apache-camel-0-0-20-release%2F&amp;#038;title=VS%20Code%20Language%20support%20for%20Apache%20Camel%200.0.20%20release" data-a2a-url="https://developers.redhat.com/blog/2019/12/16/vs-code-language-support-for-apache-camel-0-0-20-release/" data-a2a-title="VS Code Language support for Apache Camel 0.0.20 release"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/16/vs-code-language-support-for-apache-camel-0-0-20-release/"&gt;VS Code Language support for Apache Camel 0.0.20 release&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/h5rpEEmvtRo" height="1" width="1" alt=""/&gt;</content><summary>During the past months, several noticeable new features have been added to improve the developer experience of application based on Apache Camel. These updates are available in the 0.0.20 release of Visual Studio (VS) Code extension. Before going into the list of updates in detail, I want to note that I mentioned in the title the VS Code Extension release because VS Code extension is covering the ...</summary><dc:creator>Aurélien Pupier</dc:creator><dc:date>2019-12-16T14:59:48Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/16/vs-code-language-support-for-apache-camel-0-0-20-release/</feedburner:origLink></entry><entry><title>.NET Core 3.1 for Red Hat Enterprise Linux 7 now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MJNwa-w74T4/" /><category term=".net" scheme="searchisko:content:tags" /><category term=".NET Core 3.1" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Bob Davis</name></author><id>searchisko:content:id:jbossorg_blog-net_core_3_1_for_red_hat_enterprise_linux_7_now_available</id><updated>2019-12-16T14:49:31Z</updated><published>2019-12-16T14:49:31Z</published><content type="html">&lt;p&gt;We are very excited to announce the general availability of &lt;a href="https://developers.redhat.com/products/dotnet/overview"&gt;.NET Core&lt;/a&gt; 3.1 for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; 7!&lt;/p&gt; &lt;p&gt;.NET Core 3.1 offers a small number of fixes over .NET Core 3.0 and is a &lt;a href="https://access.redhat.com/support/policy/updates/net-core"&gt;long-term supported (LTS) release&lt;/a&gt;, which will be supported for three years.&lt;span id="more-667107"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;.NET Core can be installed on RHEL 7 with the usual:&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ yum install rh-dotnet31&lt;/code&gt;&lt;/p&gt; &lt;p&gt;.NET Core RHEL 7 images are available from the &lt;a href="https://catalog.redhat.com/software/containers/search?q=rh-dotnet31"&gt;Red Hat Container registry&lt;/a&gt; and can be imported in the Red Hat &lt;a href="https://www.openshift.com/"&gt;OpenShift container platform&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We recommend users of .NET Core 2.2 and 3.0 migrate their applications to .NET Core 3.1.&lt;/p&gt; &lt;p&gt;Note that .NET Core 2.2 will be end-of-life on December 23, 2019. .NET Core 3.0 will be end-of-life on March 3, 2019. The previous LTS release, .NET Core 2.1, is supported until August 21, 2021.&lt;/p&gt; &lt;p&gt;You can find more information about installing and using .NET Core 3.1 on RHEL 7 and OpenShift container platform in the &lt;a href="https://access.redhat.com/documentation/en-us/net_core/3.1/html/getting_started_guide/index"&gt;.NET Core Getting Started Guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#38;linkname=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fnet-core-3-1-for-red-hat-enterprise-linux-7-now-available%2F&amp;#038;title=.NET%20Core%203.1%20for%20Red%20Hat%20Enterprise%20Linux%207%20now%20available" data-a2a-url="https://developers.redhat.com/blog/2019/12/16/net-core-3-1-for-red-hat-enterprise-linux-7-now-available/" data-a2a-title=".NET Core 3.1 for Red Hat Enterprise Linux 7 now available"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/16/net-core-3-1-for-red-hat-enterprise-linux-7-now-available/"&gt;.NET Core 3.1 for Red Hat Enterprise Linux 7 now available&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MJNwa-w74T4" height="1" width="1" alt=""/&gt;</content><summary>We are very excited to announce the general availability of .NET Core 3.1 for Red Hat Enterprise Linux 7! .NET Core 3.1 offers a small number of fixes over .NET Core 3.0 and is a long-term supported (LTS) release, which will be supported for three years. .NET Core can be installed on RHEL 7 with the usual: $ yum install rh-dotnet31 .NET Core RHEL 7 images are available from the Red Hat Container r...</summary><dc:creator>Bob Davis</dc:creator><dc:date>2019-12-16T14:49:31Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/16/net-core-3-1-for-red-hat-enterprise-linux-7-now-available/</feedburner:origLink></entry><entry><title>Getting started with Red Hat Integration service registry</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_NM2kT1OLdM/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><category term="Red Hat Integration" scheme="searchisko:content:tags" /><author><name>Hugo Guerrero</name></author><id>searchisko:content:id:jbossorg_blog-getting_started_with_red_hat_integration_service_registry</id><updated>2019-12-16T08:00:08Z</updated><published>2019-12-16T08:00:08Z</published><content type="html">&lt;p&gt;New projects require some help. Imagine you are getting ready to start that new feature your business has been asking for the last couple of months. Your team is ready to start coding to implement the new awesome thing that would change your business.&lt;/p&gt; &lt;p&gt;To achieve it, the team will need to interact with the current existing software components of your organization. Your developers will need to interact with API services and event endpoints already available in your architecture. Before being able to send and process information, developers need to be aware of the structure or schema expected by those services.&lt;br /&gt; &lt;img class=" aligncenter size-large wp-image-663937 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/code-quarrkus-kafka-1024x440.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/code-quarrkus-kafka-1024x440.png" alt="" width="640" height="275" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/code-quarrkus-kafka-1024x440.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/code-quarrkus-kafka-300x129.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/code-quarrkus-kafka-768x330.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/"&gt;Red Hat announced&lt;/a&gt; the &lt;a href="https://access.redhat.com/support/offerings/techpreview"&gt;Technical Preview&lt;/a&gt; of the &lt;a href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; service registry to help teams to govern their services schemas. The service registry is a store for schema (and API design) artifacts providing a REST API and a set of optional rules for enforcing content validity and evolution. Teams can now use the service registry to query for the schemas required by each service endpoint or register and store new structures for future use.&lt;span id="more-661387"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Service registry overview&lt;/h2&gt; &lt;p&gt;The Red Hat Integration service registry is a datastore for standard event schemas and API designs. It enables developers to decouple the structure of their data from their applications and to share and manage their data structure using a REST interface. Red Hat service registry is built on the &lt;a href="https://github.com/apicurio/apicurio-registry"&gt;Apicurio Registry&lt;/a&gt; open source community project.&lt;/p&gt; &lt;p&gt;The service registry handles the following data formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apache Avro&lt;/li&gt; &lt;li&gt;JSON Schema&lt;/li&gt; &lt;li&gt;Protobuf (protocol buffers)&lt;/li&gt; &lt;li&gt;OpenAPI&lt;/li&gt; &lt;li&gt;AsyncAPI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can configure rules for each artifact added to the registry to govern content evolution. All rules configured for an artifact must pass before a new version can be uploaded to the registry. The goal of these rules is to prevent invalid content from being added to the registry.&lt;/p&gt; &lt;h2&gt;Using the service registry with Apache Kafka&lt;/h2&gt; &lt;p&gt;As Apache Kafka handles the actual messages value content as an opaque byte array, the usage of serialization systems is strongly suggested. &lt;a href="https://avro.apache.org/"&gt;Apache Avro&lt;/a&gt; is one of the commonly used data formats to encode Kafka data. Avro is a data serialization system that relies on schemas defined with JSON and supports schema versioning. Then Avro can convert our data based on our schema into byte arrays to send then to Kafka. Consumers can use the Avro schemas to correctly deserialize the data received.&lt;/p&gt; &lt;p&gt;The Red Hat Integration service registry provides full Kafka schema registry support to store Avro schemas. Also, the provided maven repository includes a custom Kafka client serializer/deserializer (SerDe). These utilities can be used by Kafka client developers to integrate with the registry. These Java classes allow Kafka client applications to push/pull their schemas from the service registry at runtime.&lt;/p&gt; &lt;h3&gt;Running Kafka and Registry&lt;/h3&gt; &lt;p&gt;For this example, we will use a local &lt;a href="https://docs.docker.com/compose/"&gt;docker-compose&lt;/a&gt; Kafka cluster based on Strimzi and the service registry. Service registry uses Kafka as the main data store but you can also use in-memory or JPA (currently unsupported) based stores. We will use the in-memory store to simplify the usage process. As I mentioned before, neither docker-compose nor the in-memory storage is recommended for use in production.&lt;/p&gt; &lt;p&gt;To begin with, download my preconfigured &lt;a href="https://raw.githubusercontent.com/hguerrero/amq-examples/master/registry-example-avro/docker-compose.yaml"&gt;docker-compose.yaml&lt;/a&gt; file and start the services for running locally.&lt;/p&gt; &lt;pre&gt;$ docker-compose -f docker-compose.yaml up&lt;/pre&gt; &lt;p&gt;Kafka will be running on &lt;code&gt;localhost:9092&lt;/code&gt; and the registry in &lt;code&gt;localhost:8081&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;Creating network "post_default" with the default driver Pulling zookeeper (strimzi/kafka:0.11.3-kafka-2.1.0)... … zookeeper_1 | [2019-12-09 16:56:55,407] INFO Got user-level KeeperException when processing sessionid:0x100000307a50000 type:multi cxid:0x38 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor) kafka_1 | [2019-12-09 16:56:55,408] INFO [KafkaServer id=0] started (kafka.server.KafkaServer) &lt;/pre&gt; &lt;h3&gt;Creating a new Quarkus project&lt;/h3&gt; &lt;p&gt;Now, that we started the required infrastructure, we need to create a simple client to send and consume messages to the Kafka cluster. In this scenario, I will create a simple &lt;a href="https://quarkus.io/"&gt;Quarkus&lt;/a&gt; application using the &lt;a href="https://quarkus.io/guides/kafka"&gt;MicroProfile reactive messaging&lt;/a&gt; extension for Kafka.&lt;/p&gt; &lt;p&gt;First, open a new terminal window and create a new Maven project using the Quarkus plugin:&lt;/p&gt; &lt;pre&gt;mvn io.quarkus:quarkus-maven-plugin:1.0.1.Final:create \ -DprojectGroupId=com.redhat \ -DprojectArtifactId=kafka-registry \ -Dextensions="kafka"&lt;/pre&gt; &lt;p&gt;After Maven downloads all the required artifacts you will see the “Build Success”:&lt;/p&gt; &lt;pre&gt;… [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 01:07 min [INFO] Finished at: 2019-12-09T12:17:51-05:00 [INFO] ------------------------------------------------------------------------ &lt;/pre&gt; &lt;p&gt;Open the newly created project in your preferred code editor; in my case, I will use VS Code. My editor has already installed useful extensions, like Java and Quarkus.&lt;/p&gt; &lt;p&gt;Open the &lt;code&gt;pom.xml&lt;/code&gt; and remove the &lt;code&gt;quarkus-resteasy&lt;/code&gt; dependency and add these:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupid&amp;#62;org.jboss.resteasy&amp;#60;/groupid&amp;#62; &amp;#60;artifactid&amp;#62;resteasy-jackson2-provider&amp;#60;/artifactid&amp;#62; &amp;#60;/dependency&amp;#62; &amp;#60;dependency&amp;#62; &amp;#60;groupid&amp;#62;io.apicurio&amp;#60;/groupid&amp;#62; &amp;#60;artifactid&amp;#62;apicurio-registry-utils-serde&amp;#60;/artifactid&amp;#62; &amp;#60;version&amp;#62;1.0.2.Final&amp;#60;/version&amp;#62; &amp;#60;/dependency&amp;#62; &lt;/pre&gt; &lt;p&gt;Create the following Java class under &lt;code&gt;src/main/java/com/redhat/kafka/registry/AvroRegistryExample.java&lt;/code&gt; and add the following code:&lt;/p&gt; &lt;pre&gt;package com.redhat.kafka.registry; import java.io.File; import java.io.IOException; import java.util.Random; import java.util.concurrent.TimeUnit; import javax.enterprise.context.ApplicationScoped; import org.apache.avro.Schema; import org.apache.avro.generic.GenericData; import org.apache.avro.generic.GenericData.Record; import org.eclipse.microprofile.reactive.messaging.Outgoing; import io.reactivex.Flowable; import io.smallrye.reactive.messaging.kafka.KafkaMessage; @ApplicationScoped public class AvroRegistryExample { private Random random = new Random(); private String[] symbols = new String[] { "RHT", "IBM", "MSFT", "AMZN" }; @Outgoing("price-out") public Flowable&amp;#60;KafkaMessage&amp;#60;String, Record&amp;#62;&amp;#62; generate() throws IOException { Schema schema = new Schema.Parser().parse( new File(getClass().getClassLoader().getResource("price-schema.avsc").getFile()) ); return Flowable.interval(1000, TimeUnit.MILLISECONDS) .onBackpressureDrop() .map(tick -&amp;#62; { Record record = new GenericData.Record(schema); record.put("symbol", symbols[random.nextInt(4)]); record.put("price", String.format("%.2f", random.nextDouble() * 100)); return KafkaMessage.of(record.get("symbol").toString(), record); }); } }&lt;/pre&gt; &lt;p&gt;In the code, we are instructing the reactive messaging extension to send items from the stream to the &lt;code&gt;price-out&lt;/code&gt; through the &lt;code&gt;Outgoing&lt;/code&gt; annotation. The stream out is a &lt;code&gt;Flowable&lt;/code&gt; RX Java 2 stream emitting new stock prices every 1 second.&lt;/p&gt; &lt;h3&gt;Working with Schemas&lt;/h3&gt; &lt;p&gt;As you might notice we need the Avro schema for this stock ticker to correctly format the message, so we will create a simple one under &lt;code&gt;src/main/resources/price-schema.avsc&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt;{ "type": "record", "name": "price", "namespace": "com.redhat", "fields": [ { "name": "symbol", "type": "string" }, { "name": "price", "type": "string" } ] }&lt;/pre&gt; &lt;p&gt;In the previous file, we specified the &lt;code&gt;symbol&lt;/code&gt; and the &lt;code&gt;price&lt;/code&gt; fields to be included in the Avro record.&lt;/p&gt; &lt;p&gt;We need to let the registry know that this is the schema we will be validating to every time we send a message to the Kafka Topic. To archive this, we will use the REST API provided by the registry to add the schema.&lt;/p&gt; &lt;p&gt;First, we will create a new artifact with type &lt;code&gt;AVRO&lt;/code&gt; by doing a POST call to the API using cURL. Remove spaces and format from the avro schema file to have a canonical version.&lt;/p&gt; &lt;pre&gt;curl -X POST -H "Content-type: application/json; artifactType=AVRO" -H "X-Registry-ArtifactId: prices-value" --data '{"type":"record","name":"price","namespace":"com.redhat","fields":[{"name":"symbol","type":"string"},{"name":"price","type":"string"}]}' http://localhost:8081/artifacts -s | jq &lt;/pre&gt; &lt;p&gt;This call will create a new artifact with &lt;code&gt;prices-value&lt;/code&gt; as id. The rest of the headers are used to identify the schema as an AVRO schema and to indicate we are using JSON as the payload type.&lt;/p&gt; &lt;pre&gt;{ "createdOn": 1575919739708, "modifiedOn": 1575919739708, "id": "prices-value", "version": 1, "type": "AVRO", "globalId": 4 } &lt;/pre&gt; &lt;h3&gt;Configuration&lt;/h3&gt; &lt;p&gt;Next, we need to configure the Kafka connector. This is done in the application properties file. So open the file under &lt;code&gt;src/main/resources/application.properties&lt;/code&gt; and fill it out with the following configurations:&lt;/p&gt; &lt;pre&gt;# Configuration file kafka.bootstrap.servers=localhost:9092 mp.messaging.outgoing.price-out.connector=smallrye-kafka mp.messaging.outgoing.price-out.client.id=price-producer mp.messaging.outgoing.price-out.topic=prices mp.messaging.outgoing.price-out.key.serializer=org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.price-out.value.serializer=io.apicurio.registry.utils.serde.AvroKafkaSerializer mp.messaging.outgoing.price-out.apicurio.registry.url=http://localhost:8081 mp.messaging.outgoing.price-out.apicurio.registry.artifact-id=io.apicurio.registry.utils.serde.strategy.TopicIdStrategy&lt;/pre&gt; &lt;p&gt;In the previous file we indicate that we will be connecting to the localhost Kafka cluster running on port 9092, and configured the messaging outgoing channel &lt;code&gt;price-out&lt;/code&gt; connector using smallrye-kafka extension with a StringSerializer for the key and a &lt;code&gt;io.apicurio.registry.utils.serde.AvroKafkaSerializer&lt;/code&gt; class for the value.&lt;/p&gt; &lt;p&gt;This configuration will enable us to use the Apicurio SerDe for managing access to the registry to validate the schema for our Avro record.&lt;/p&gt; &lt;p&gt;The last two rows indicate where the registry is listening and the type of strategy used for the schema retrieval. In our example, we are using a &lt;code&gt;TopicIdStrategy&lt;/code&gt; meaning we will search for artifacts with the same name as the Kafka topic we are sending our messages to.&lt;/p&gt; &lt;h3&gt;Running the application&lt;/h3&gt; &lt;p&gt;If you are ready is time to get the application running. For that, you just need to run the following command:&lt;/p&gt; &lt;pre&gt;./mvnw compile quarkus:dev &lt;/pre&gt; &lt;p&gt;You will see in the log that your application is now sending messages to Kafka.&lt;/p&gt; &lt;pre&gt;2019-12-09 14:30:58,007 INFO [io.sma.rea.mes.ext.MediatorManager] (main) Initializing mediators 2019-12-09 14:30:58,203 INFO [io.sma.rea.mes.ext.MediatorManager] (main) Connecting mediators 2019-12-09 14:30:58,206 INFO [io.sma.rea.mes.ext.MediatorManager] (main) Connecting method com.redhat.kafka.registry.AvroRegistryExample#generate to sink price-out 2019-12-09 14:30:58,298 INFO [io.quarkus] (main) Quarkus 1.0.1.Final started in 1.722s. 2019-12-09 14:30:58,301 INFO [io.quarkus] (main) Profile dev activated. Live Coding activated. 2019-12-09 14:30:58,301 INFO [io.quarkus] (main) Installed features: [cdi, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, smallrye-reactive-streams-operators] 2019-12-09 14:30:58,332 INFO [org.apa.kaf.cli.Metadata] (kafka-producer-network-thread | price-producer) Cluster ID: B2U0Vs6eQS-kjJG3_L2tCA 2019-12-09 14:30:59,309 INFO [io.sma.rea.mes.kaf.KafkaSink] (RxComputationThreadPool-1) Sending message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@12021771 to Kafka topic 'prices' 2019-12-09 14:31:00,083 INFO [io.sma.rea.mes.kaf.KafkaSink] (vert.x-eventloop-thread-0) Message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@12021771 sent successfully to Kafka topic 'prices' 2019-12-09 14:31:00,297 INFO [io.sma.rea.mes.kaf.KafkaSink] (RxComputationThreadPool-1) Sending message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@5e5f389a to Kafka topic 'prices' 2019-12-09 14:31:00,334 INFO [io.sma.rea.mes.kaf.KafkaSink] (vert.x-eventloop-thread-0) Message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@5e5f389a sent successfully to Kafka topic 'prices' 2019-12-09 14:31:01,301 INFO [io.sma.rea.mes.kaf.KafkaSink] (RxComputationThreadPool-1) Sending message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@5a403106 to Kafka topic 'prices' 2019-12-09 14:31:01,341 INFO [io.sma.rea.mes.kaf.KafkaSink] (vert.x-eventloop-thread-0) Message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@5a403106 sent successfully to Kafka topic 'prices' 2019-12-09 14:31:02,296 INFO [io.sma.rea.mes.kaf.KafkaSink] (RxComputationThreadPool-1) Sending message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@3bb2aac0 to Kafka topic 'prices' 2019-12-09 14:31:02,323 INFO [io.sma.rea.mes.kaf.KafkaSink] (vert.x-eventloop-thread-0) Message io.smallrye.reactive.messaging.kafka.SendingKafkaMessage@3bb2aac0 sent successfully to Kafka topic 'prices' &lt;/pre&gt; &lt;p&gt;The messages you are sending to Kafka are using the Apicurio serializer to validate the record schema using Red Hat Integration service registry. If you want to take a closer look at the code and see how to implement the &lt;code&gt;Incoming&lt;/code&gt; pattern for Quarkus, take a took at the full example in my &lt;a href="https://github.com/hguerrero/amq-examples"&gt;amq-examples GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To make it easy to transition from Confluent, the service registry also adds compatibility with the Confluent Schema Registry REST API. This means that applications using Confluent client libraries can replace Schema Registry and use Red Hat Integration service registry instead.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;The Red Hat Integration service registry is a central data store for schemas and API artifacts. Developers can query, create, read, update, and delete service artifacts, versions, and rules to govern the structure of their services. Developer teams can work with popular formats like Avro or Protobuf schemas as well as OpenAPI and AsyncAPI definitions. The service registry could be also used as a drop-in replacement for Confluent registry with Apache Kafka clients by using the included serializer and deserializer classes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;See also:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/"&gt;Red Hat advances Debezium CDC connectors for Apache Kafka support to Technical Preview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/26/red-hat-simplifies-transition-to-open-source-kafka-with-new-service-registry-and-http-bridge/"&gt;Red Hat simplifies transition to open source Kafka with new service registry and HTTP bridge&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F16%2Fgetting-started-with-red-hat-integration-service-registry%2F&amp;#038;title=Getting%20started%20with%20Red%20Hat%20Integration%20service%20registry" data-a2a-url="https://developers.redhat.com/blog/2019/12/16/getting-started-with-red-hat-integration-service-registry/" data-a2a-title="Getting started with Red Hat Integration service registry"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/16/getting-started-with-red-hat-integration-service-registry/"&gt;Getting started with Red Hat Integration service registry&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_NM2kT1OLdM" height="1" width="1" alt=""/&gt;</content><summary>New projects require some help. Imagine you are getting ready to start that new feature your business has been asking for the last couple of months. Your team is ready to start coding to implement the new awesome thing that would change your business. To achieve it, the team will need to interact with the current existing software components of your organization. Your developers will need to inter...</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2019-12-16T08:00:08Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/16/getting-started-with-red-hat-integration-service-registry/</feedburner:origLink></entry><entry><title>Jakarta EE: Creating an Enterprise JavaBeans timer</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rjom_Czg8Ho/" /><category term="Enterprise JavaBeans" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><author><name>rhsilva</name></author><id>searchisko:content:id:jbossorg_blog-jakarta_ee_creating_an_enterprise_javabeans_timer</id><updated>2019-12-13T08:00:49Z</updated><published>2019-12-13T08:00:49Z</published><content type="html">&lt;p&gt;&lt;a href="https://www.javaworld.com/article/2076777/a-beginner-s-guide-to-enterprise-javabeans.html"&gt;Enterprise JavaBeans&lt;/a&gt; (EJB) has many interesting and useful features, some of which I will be highlighting in this and upcoming articles. In this article, I&amp;#8217;ll show you how to create an&lt;a href="https://docs.oracle.com/cd/E16439_01/doc.1013/e13981/undejdev012.htm"&gt; EJB timer&lt;/a&gt; programmatically and with annotation. Let&amp;#8217;s go!&lt;/p&gt; &lt;p&gt;The EJB timer feature allows us to schedule tasks to be executed according a calendar configuration. It is very useful because we can execute scheduled tasks using the power of &lt;a href="https://developers.redhat.com/blog/2019/09/12/jakarta-ee-8-the-new-era-of-java-ee-explained/"&gt;Jakarta&lt;/a&gt; context. When we run tasks based on a timer, we need to answer some questions about concurrency, which node the task was scheduled on (in case of an application in a cluster), what is the action if the task does not execute, and others. When we use the EJB timer we can delegate many of these concerns to Jakarta context and care more about business logic. It is interesting, isn&amp;#8217;t it?&lt;span id="more-652897"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Creating an EJB timer programmatically&lt;/h2&gt; &lt;p&gt;We can schedule an EJB timer to runs according to a business logic using a programmatic approach. This method can be used when we want a dynamic behavior, according to the parameter values passed to the process. Let&amp;#8217;s look at an example of an EJB timer:&lt;/p&gt; &lt;pre&gt;import javax.annotation.Resource; import javax.ejb.SessionContext; import javax.ejb.Stateless; import javax.ejb.Timeout; import java.util.logging.Logger; @Stateless public class MyTimer { private Logger logger = Logger.getLogger(MyTimer.class.getName()); @Resource private SessionContext context; public void initTimer(String message){ context.getTimerService().createTimer(10000, message); } @Timeout public void execute(){ logger.info("Starting"); context.getTimerService().getAllTimers().stream().forEach(timer -&amp;#62; logger.info(String.valueOf(timer.getInfo()))); logger.info("Ending"); } } &lt;/pre&gt; &lt;p&gt;To schedule this EJB timer, call this method:&lt;/p&gt; &lt;pre&gt;@Inject private MyTimer myTimer; ....&lt;/pre&gt; &lt;pre&gt;myTimer.initTimer(message);&lt;/pre&gt; &lt;p&gt;After passing 10000 milliseconds, the method annotated with @Timeout will be called.&lt;/p&gt; &lt;h2&gt;Scheduling an EJB timer using annotation&lt;/h2&gt; &lt;p&gt;We can also create an EJB timer that is automatically scheduled to run according to an annotation configuration. Look at this example:&lt;/p&gt; &lt;pre&gt;@Singleton public class MyTimerAutomatic { private Logger logger = Logger.getLogger(MyTimerAutomatic.class.getName()); @Schedule(hour = "*", minute = "*",second = "0,10,20,30,40,50",persistent = false) public void execute(){ logger.info("Automatic timer executing"); } } &lt;/pre&gt; &lt;p&gt;As you can see, to configure an automatic EJB timer schedule, you can annotate the method using @Schedule and configure the calendar attributes. For example:&lt;/p&gt; &lt;pre&gt;@Schedule(hour = "*", minute = "*",second = "0,10,20,30,40,50",persistent = false)&lt;/pre&gt; &lt;p&gt;As you can see, the method execute is configured to be called every 10 seconds. You can configure whether the timer is persistent as well.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;EJB timer is a good EJB feature that is helpful in solving many problems. Using the EJB timer feature, we can schedule tasks to be executed, thereby delegating some responsibilities to Jakarta context to solve for us. Furthermore, we can create persistent timers, control the concurrent execution, and work with it in a clustered environment.  If you want to see the complete example, visit this &lt;a href="https://github.com/rhuan080/sampleejbtime"&gt;repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#38;linkname=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fjakarta-ee-creating-an-enterprise-javabeans-timer%2F&amp;#038;title=Jakarta%20EE%3A%20Creating%20an%20Enterprise%20JavaBeans%20timer" data-a2a-url="https://developers.redhat.com/blog/2019/12/13/jakarta-ee-creating-an-enterprise-javabeans-timer/" data-a2a-title="Jakarta EE: Creating an Enterprise JavaBeans timer"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/13/jakarta-ee-creating-an-enterprise-javabeans-timer/"&gt;Jakarta EE: Creating an Enterprise JavaBeans timer&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rjom_Czg8Ho" height="1" width="1" alt=""/&gt;</content><summary>Enterprise JavaBeans (EJB) has many interesting and useful features, some of which I will be highlighting in this and upcoming articles. In this article, I’ll show you how to create an EJB timer programmatically and with annotation. Let’s go! The EJB timer feature allows us to schedule tasks to be executed according a calendar configuration. It is very useful because we can execute scheduled tasks...</summary><dc:creator>rhsilva</dc:creator><dc:date>2019-12-13T08:00:49Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/13/jakarta-ee-creating-an-enterprise-javabeans-timer/</feedburner:origLink></entry><entry><title>We’re headed for edge computing</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6yJe5tZCQ0E/" /><category term="cloud" scheme="searchisko:content:tags" /><category term="edge deployment" scheme="searchisko:content:tags" /><category term="edge development" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Internet of Things" scheme="searchisko:content:tags" /><author><name>Ishu Verma</name></author><id>searchisko:content:id:jbossorg_blog-we_re_headed_for_edge_computing</id><updated>2019-12-13T08:00:35Z</updated><published>2019-12-13T08:00:35Z</published><content type="html">&lt;p&gt;Every week seems to bring a new report on how &lt;a href="https://www.redhat.com/en/topics/edge-computing/what-is-edge-computing"&gt;edge computin&lt;/a&gt;g is going to take over the world. This crescendo has been building for the past few years, so it’s no surprise that edge computing sits near the peak on the &lt;a href="https://www.gartner.com/en/documents/3956137/hype-cycle-for-edge-computing-2019"&gt;Gartner hype cycle&lt;/a&gt; for emerging technologies. But the question remains—will the edge computing phenomenon take over the world as predicted and, if so, how can businesses benefit from it?&lt;/p&gt; &lt;p&gt;In this and future articles, we’ll demystify edge computing, examine its motivations, and explore best practices in creating scalable edge deployments and the role of open source at the edge. We&amp;#8217;ll also look at 5G and its impact to the telco industry, remote office/branch office, IoT, and other use cases.&lt;/p&gt; &lt;h2&gt;What is edge computing?&lt;/h2&gt; &lt;p&gt;Depending on the industry or use case, the term edge computing (EC) has been used to describe everything from actions performed by tiny IoT devices to datacenter-like infrastructure. Terms used to denote edge computing include: distributed computing, hybrid edge computing, heterogeneous computing, matrix computing, datacenter-in-a-box, local cloud, network edge, fog computing, and more. Depending on the industry, the meaning of each of these terms is imbued with its own perspective.&lt;/p&gt; &lt;p&gt;To add to the confusion, there is not a single edge, but a continuum of edge tiers with different properties in terms of distance to users, number of sites, size of sites, ownership, etc. The location of where edge computing is located is subject to interpretation. For service providers, EC can extend from core to the last mile, whereas for enterprises, EC is located on-premise.&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;em&gt;At the conceptual level, edge computing refers to the idea of bringing computing closer to where it&amp;#8217;s consumed or closer to the sources of data. This concept is not limited to computing services but could also include networking or storage services.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;The debate over where computing resources should be located is perhaps as old as computing itself. The pendulum often swings between efficiencies and economies of scale offered by centralized computing to flexibility and user control offered by non-centralized computing. Past trends have included client/server, PC vs. mainframe, etc.&lt;/p&gt; &lt;p&gt;The edge computing concept is more than two decades old, pioneered by Akamai for &lt;a href="https://en.wikipedia.org/wiki/Content_delivery_network"&gt;Content Delivery Networks&lt;/a&gt; (CDNs) whereby frequently accessed content is cached closer to end users. In the present context, the edge computing scope is much broader, encompassing businesses, consumers, and service providers.&lt;/p&gt; &lt;p&gt;There is great variability across various EC use cases and industries, where every use case presents its own unique requirements for edge. For an IoT use case, how the edge operates is different from a remote site like a windmill or autonomous vehicle and also different from the requirements of a factory or stadium. For example, a remote site that has constraints on computing infrastructure and network bandwidth mostly operates in offline mode, whereas a stadium would have a mini datacenter-like infrastructure with broadband connectivity.&lt;/p&gt; &lt;h2&gt;Why is computing moving to the edge?&lt;/h2&gt; &lt;p&gt;In the past decade, the shift to cloud services has resulted in computing resources being concentrated in a few large datacenters. Edge computing is a counter-trend that decentralizes cloud services and distributes them to many sites that are located closer to end users or data sources. It allows applications to deliver a better quality experience, thereby also enabling new use cases and gaining operational efficiencies. The main reasons for EC can be categorized into areas of bandwidth, latency, resiliency, and security.&lt;/p&gt; &lt;p&gt;Some emerging use cases, like IoT or video surveillance, are expected to generate huge amounts of data (100s of GB/day) and have constrained network connectivity via cellular/satellite (e.g., offshore oil platform, ship at sea). By processing data closer to the data source, EC can help reduce network bandwidth required to move device data to back-end systems. The majority of device data could be redundant information. Think of room temperature data from a thermostat, for example, that could be processed locally with only a small aggregated dataset being sent to back-end systems.&lt;/p&gt; &lt;p&gt;For use cases like mobile AR/VR with edge-based rendering or autonomous driving with real-time decision making, the latency introduced by communicating with a centralized site over a long distance, could impact user experience or safety. EC helps reduce latencies and becomes a key requirement for time-sensitive use cases.&lt;/p&gt; &lt;p&gt;For critical business functions, edge computing provides resiliency for service continuity despite intermittent network connectivity (e.g., autonomous vehicles, smart buildings, agriculture). By limiting the affected areas of service failures to a smaller service area (e.g., mobile edge computing), it provides for greater resiliency. EC also allows for better data sovereignty by keeping sensitive information close to its source for security or regulatory reasons.&lt;/p&gt; &lt;p&gt;It’s not an either/or choice between edge computing and centralized computing. As EC gains greater adoption in the marketplace, the overall solution would encompass a combination of the two. In such a hybrid computing model, centralized computing would be used for compute-intensive workloads, data aggregation and storage, AI/machine learning, coordinating operations across geographies, and traditional back-end processing. Edge computing, on the other hand, could help solve problems at the source, in near real time.&lt;/p&gt; &lt;p&gt;Architects will need to identify use cases that are aligned with edge computing. If a use case doesn’t benefit from reduced latency, real-time monitoring, or other attributes, the customer may not find edge computing appealing.&lt;/p&gt; &lt;h2&gt;Who is using edge computing?&lt;/h2&gt; &lt;p&gt;Emerging use cases like IoT, AI/ML, AR/VR, robotics, and telco network functions are often cited as key drivers to move computing to the edge. However, traditional enterprises are also starting to adopt this approach in order to better support their remote/branch offices, retail locations, manufacturing plants, etc. Even cloud service providers have recognized the need for processing data closer to source and are offering edge solutions.&lt;/p&gt; &lt;p&gt;Analysts are forecasting a data gravity shift from core datacenters out to the edge. By 2021, consumer-facing industries will annually spend more on the network, computing, and storage resources in edge locations than on upgrades in core datacenters.&lt;/p&gt; &lt;p&gt;According to &lt;a href="https://www.idc.com/research/viewtoc.jsp?containerId=US43152417"&gt;one report by IDC&lt;/a&gt;, by 2022, local cloud offerings will account for a quarter of all hosted private cloud spending. Companies like eBay are decentralizing their cluster of data centers to create a faster, more consistent user experience, &lt;a href="https://www.datacenterknowledge.com/ebay/ebay-designs-own-servers-decentralizes-data-center-strategy"&gt;saving 600-800 milliseconds of load time&lt;/a&gt; by deploying online services and data closer to their users, enabling dynamic and static caching capabilities.&lt;/p&gt; &lt;p&gt;For companies looking for low-latency or disconnected computing, where remote sites can operate without communication with centralized infrastructure, EC can help improve the infrastructure resiliency and application availability. Edge computing can similarly benefit a large number of use cases including utilities, transportation, healthcare, industrial, energy, and retail.&lt;/p&gt; &lt;p&gt;For service providers, EC can help improve the quality of experience of their customers by moving applications or content towards the edge tiers in the network hierarchy. They can also deploy an entirely new class of services in the edge to take advantage of their proximity to the customers. As network edge represents a majority of the operator’s capital and operational expenses, it is also a key area of interest for network modernization efforts.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on edge computing, &lt;a href="https://www.redhat.com/en/topics/edge-computing"&gt;click here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#38;linkname=We%E2%80%99re%20headed%20for%20edge%20computing" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F13%2Fwere-headed-for-edge-computing%2F&amp;#038;title=We%E2%80%99re%20headed%20for%20edge%20computing" data-a2a-url="https://developers.redhat.com/blog/2019/12/13/were-headed-for-edge-computing/" data-a2a-title="We’re headed for edge computing"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/13/were-headed-for-edge-computing/"&gt;We&amp;#8217;re headed for edge computing&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6yJe5tZCQ0E" height="1" width="1" alt=""/&gt;</content><summary>Every week seems to bring a new report on how edge computing is going to take over the world. This crescendo has been building for the past few years, so it’s no surprise that edge computing sits near the peak on the Gartner hype cycle for emerging technologies. But the question remains—will the edge computing phenomenon take over the world as predicted and, if so, how can businesses benefit from ...</summary><dc:creator>Ishu Verma</dc:creator><dc:date>2019-12-13T08:00:35Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/13/were-headed-for-edge-computing/</feedburner:origLink></entry></feed>
